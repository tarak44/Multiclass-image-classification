{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<p>\n",
    "  <b>AI Lab: Deep Learning for Computer Vision</b><br>\n",
    "  <b><a href=\"https://www.wqu.edu/\">WorldQuant University</a></b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <p>\n",
    "    <center><b>Usage Guidelines</b></center>\n",
    "  </p>\n",
    "  <p>\n",
    "    This file is licensed under <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International</a>.\n",
    "  </p>\n",
    "  <p>\n",
    "    You <b>can</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">✓</span> Download this file</li>\n",
    "      <li><span style=\"color: green\">✓</span> Post this file in public repositories</li>\n",
    "    </ul>\n",
    "    You <b>must always</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">✓</span> Give credit to <a href=\"https://www.wqu.edu/\">WorldQuant University</a> for the creation of this file</li>\n",
    "      <li><span style=\"color: green\">✓</span> Provide a <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">link to the license</a></li>\n",
    "    </ul>\n",
    "    You <b>cannot</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: red\">✗</span> Create derivatives or adaptations of this file</li>\n",
    "      <li><span style=\"color: red\">✗</span> Use this file for commercial purposes</li>\n",
    "    </ul>\n",
    "  </p>\n",
    "  <p>\n",
    "    Failure to follow these guidelines is a violation of your terms of service and could lead to your expulsion from WorldQuant University and the revocation your certificate.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll import the packages we'll need in this notebook. Most of these are the same as the previous notebook, but there are a few new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the versions of our packages again. If we come back to this later, we'll know what we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Platform:\", sys.platform)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"---\")\n",
    "print(\"matplotlib version:\", matplotlib.__version__)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"PIL version:\", PIL.__version__)\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"torchvision version:\", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be running on GPUs, so the device should be `cuda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to read in our data. Since we'll be using images once again, we'll need to convert them to something our network can understand. To start with, we'll use the same set of transformations we used in the previous notebook.\n",
    "\n",
    "These transformations are\n",
    "- Convert any grayscale images to RGB format with a custom class\n",
    "- Resize the image, so that they're all the same size (we chose $224$ x $224$, but other sizes would work as well)\n",
    "- Convert the image to a Tensor of pixel values\n",
    "\n",
    "This should result in each image becoming a Tensor of size $3$ x $224$ x $224$. We'll check this once we read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToRGB:\n",
    "    def __call__(self, img):\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        ConvertToRGB(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we were working with only two categories. That data was in the `data_binary` subdirectory. Here we'll work with all eight categories, in the `data_multiclass` subdirectory. Let's load that data. We will follow the same pattern we used in the last notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.1:** Assign the path to the multi-class training data to `train_dir`. Then use the `ImageFolder` tool to open those files and apply our transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = ...\n",
    "train_dir = ...\n",
    "\n",
    "print(\"Will read data from\", train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, let's verify that we got what we wanted. We should have classes for each of the seven animals, and one `'blank'` for when there wasn't an animal in the image. Additionally, the tensors we get should be of size $3$ x $224$ x $224$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classes:\")\n",
    "print(dataset.classes)\n",
    "print(f\"That's {len(dataset.classes)} classes\")\n",
    "print()\n",
    "print(\"Tensor shape for one image:\")\n",
    "print(dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, we could work with the data like this. But PyTorch is expecting the data to be broken into batches with a `DataLoader`. This prevents PyTorch from trying to load all of the files into memory at once, which would cause our notebook to crash. Instead, it loads just a few (the `batch_size`), works with them, then discards them. Since all the tools are expecting it, we should convert ours. The batch size to work with will depend on our system, but something in the $20$ to $100$ range is usually fine. We'll pick $32$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# Get one batch\n",
    "first_batch = next(iter(dataset_loader))\n",
    "\n",
    "print(f\"Shape of one batch: {first_batch[0].shape}\")\n",
    "print(f\"Shape of labels: {first_batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we loop over this loader, it'll produce small batches of our images. This is what we want — these are the \"minibatches\" that will speed up our computations. In our case, each batch is $32$ images, with each image $3$ x $224$ x $224$. It also provides us with the labels for the correct answers. This is the information we need to train a network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "We're going to be cheating a bit here. Technically, we should divide our data into a training set and validation set right now. Then we should do all of our work on just the training set. This prevents information from the validation set leaking into the training set. But the way that PyTorch organizes its data loading makes that much more difficult than what we're going to do. We're going to do one simple transformation to our data, then do the test-validation split. If we were doing anything more complicated, we'd have to be more careful.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we were reading in the data, we already did some preparation. Our images are all the same shape, and have been converted to tensors. But neural networks tend to perform best with data that has a mean of $0$ and a standard deviation of $1$. Data that has that property is called _normalized_. In our case, that would be the mean and standard deviation of all of the pixels in all of the images.\n",
    "\n",
    "Let's see what they are for our data. Here's a function that computes the mean and standard deviation for each color channel (red, green, and blue) separately. It takes in a `DataLoader` and returns the mean and standard deviation of each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(loader):\n",
    "    \"\"\"Computes the mean and standard deviation of image data.\n",
    "\n",
    "    Input: a `DataLoader` producing tensors of shape [batch_size, channels, pixels_x, pixels_y]\n",
    "    Output: the mean of each channel as a tensor, the standard deviation of each channel as a tensor\n",
    "            formatted as a tuple (means[channels], std[channels])\"\"\"\n",
    "\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    for data, _ in tqdm(loader, desc=\"Computing mean and std\", leave=False):\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "    mean = channels_sum / num_batches\n",
    "    std = (channels_squared_sum / num_batches - mean**2) ** 0.5\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.2:** Run the `get_mean_std` function. on the training data, and save the means and standard deviations to variables `mean` and `std`. There should be a value for each color channel, giving us vectors of length $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = ...\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard deviation: {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these values, we can adjust our pixels. To get mean $0$, we need to subtract our measured `mean` from every pixel. To get standard deviation $1$, we divide every pixel by the `std`. \n",
    "\n",
    "We can perform these calculations using the `Normalize` transformation that `torchvision` gives us. We'll add it as an extra step to the transform we created earlier. Since we're doing the three color channels separately, we'll nave to give `Normalize` a vector with three means (and the same for standard deviation). Conveniently, that's what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_norm = transforms.Compose(\n",
    "    [\n",
    "        ConvertToRGB(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.3:** Make a new normalized dataset using `ImageFolder` and a new `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dataset = ...\n",
    "\n",
    "norm_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this did what we wanted, the normalized data should have mean $0$ and standard deviation $1$ in each color channel.\n",
    "\n",
    "**Task 1.4.4:** Use the `get_mean_std` function to verify the mean and standard deviation are correct in the `norm_loader` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mean, norm_std = ...\n",
    "\n",
    "print(f\"Mean: {norm_mean}\")\n",
    "print(f\"Standard deviation: {norm_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The means may not be _exactly_ zero due to machine precision. But they should be extremely small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "This sort of \"rounding error\" is extremely common when working with floating point numbers on a computer. The computer only stores a certain number of digits after the decimal point. This rounding means that math operations sometimes don't get the last few digits right. This becomes very obvious when you subtract two numbers that should be the same. If the last digits in the two numbers are different because of this rounding, you won't get zero, you'll get that last digit leftover.\n",
    "\n",
    "As an example, $\\frac{1}{3} - \\frac{1}{5} = \\frac{2}{15}$. But the following cell doesn't quite give you zero.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / 3 - 1 / 5 - 2 / 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validation splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we'll need to divide our data into two parts. We'll train our data on some of our images, and reserve some of them for validation. This will let us see how our model does on images it hasn't seen before. If it makes good predictions on the training data but not on the validation data, we'll know it has overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.5:** Use `random_split` to create a training dataset with 80% of the data, and a validation dataset with 20% of the data. Be sure to use the normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important, don't change this!\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "train_dataset, val_dataset = ...\n",
    "\n",
    "length_train = len(train_dataset)\n",
    "length_val = len(val_dataset)\n",
    "length_dataset = len(norm_dataset)\n",
    "percent_train = np.round(100 * length_train / length_dataset, 2)\n",
    "percent_val = np.round(100 * length_val / length_dataset, 2)\n",
    "\n",
    "print(f\"Train data is {percent_train}% of full data\")\n",
    "print(f\"Validation data is {percent_val}% of full data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the training data and validation data to be similar. Ideally, the random split should do that for us. But with multiple categories, it's possible we have too much of one category ending up in training or validation.  For example, we could have gotten really unlucky and wound up with all of our leopards in the validation set. \n",
    "\n",
    "We should check how many of each category we have in each of our data sets. We aren't expecting the same _number_ of each. The different categories started with different amounts, and the training set is $4$ times as big. But we should expect the training and validation sets to have the same _proportion_ of each category.\n",
    "\n",
    "As we did in the last notebook, we can visualize this with a bar chart. We'll use the same function as last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_counts(dataset):\n",
    "    c = Counter(x[1] for x in tqdm(dataset))\n",
    "    class_to_index = dataset.dataset.class_to_idx\n",
    "    return pd.Series({cat: c[idx] for cat, idx in class_to_index.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it to see how many of each category in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_distributions = class_counts(train_dataset)\n",
    "\n",
    "train_class_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful, but visualizations are much easier for humans to understand. Let's make that bar plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.6:** Create a bar plot from these counts. We did this in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot from train_class_distribution\n",
    "\n",
    "\n",
    "# Add axis labels and title\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.ylabel(\"Frequency [count]\")\n",
    "plt.title(\"Class Distribution in Training Set\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can repeat this for the validation set and compare them by eye."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.7:** Make the same graph for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class distribution\n",
    "validation_class_distributions = ...\n",
    "\n",
    "# Create a bar plot from train_class_distribution\n",
    "\n",
    "\n",
    "# Add axis labels and title\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.ylabel(\"Frequency [count]\")\n",
    "plt.title(\"Class Distribution in Validation Set\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two graphs should look similar, though they won't be identical. The random process always produces some differences. If they are _too_ different, you can run your train-validation split again to get a better balance. If you do this, remake the graphs to make sure they're actually better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "With a little bit more work, you can put the two bar graphs together into one graph. This would make them easier to compare. You could do this by combining the two pandas Series into one DataFrame. But if you do, make sure you change the counts into fractions — we have many more points in our training set! The easiest way to do that is to divide each Series by its total counts (its sum).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're happy with our split, we'll need to make a `DataLoader` again. We'll need separate ones for the training and validation sets. But for the training data, we'll want it to be shuffled every time we run it. As in the previous notebook, we'll add `shuffle=True` to the arguments. We won't want the validation set shuffled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.8:** Create data loaders for the train and validation sets. Use a batch size of $32$. Be sure to turn on shuffling for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important, don't change this!\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "\n",
    "batch_size = ...\n",
    "\n",
    "train_loader = ...\n",
    "\n",
    "val_loader = ...\n",
    "\n",
    "single_batch = next(iter(train_loader))[0]\n",
    "print(f\"Shape of one batch: {single_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Better Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data in hand, we're ready to address our task. As in the last notebook, we want a model that will take a picture, and tell us what's in it. But this time, instead of just \"hog\" or \"not hog\", we want it to tell us which animal is in the image. This makes it a _multiclass_ classification problem.\n",
    "\n",
    "Now we're going to have our model give a confidence for _each_ class. This will give us $8$ values. As before, higher numbers mean that the model is more confident that class is the right answer. A possible output for a single image would be the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_confidence = torch.tensor([0.13, 0.01, 0.02, 0.12, 0.10, 0.34, 0.16, 0.12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction in this case would be the category with the highest confidence. These categories appear in the order set in the dataset. We can find which one is the largest using `argmax`, as we did in the previous notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.9:** Find which animal we should predict, using `argmax` and the list of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = norm_dataset.classes\n",
    "\n",
    "class_number = ...\n",
    "prediction = ...\n",
    "\n",
    "print(f\"This image is a {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "You may notice that these numbers sum to $1$ (try it!). Most tools for doing machine learning produce outputs like this. You will often hear this referred to as the \"probability\" of each class. That the model thinks there's a $13\\%$ chance it's class $0$, a $1\\%$ chance it's a class $1$, etc.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our training data, we also already know the correct class. Our model will generate a set of $8$ confidence values for each image, which we can use to make predictions. We can compare these predictions to the right answer to find out how well the model has done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a model. We'll build another neural network. The one we built last time was a shallow neural network. This is a network made of a sequence of \"dense\" or \"fully connected\" layers. But that's not the only option! You can connect the neurons in different ways, to get different _architectures_.\n",
    "\n",
    "The shallow neural network is the \"standard\", most common one. We could use it for our problem, but to get good accuracy we'd need to add more layers. That network would take a long time to train. We'll be better served by using an architecture meant for images. \n",
    "\n",
    "A good option, and the one we'll use, is the convolutional neural network (CNN). It consists of a sequence of convolutional and max pooling layers. These are usually followed by some fully connected layers and an output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN gets its name from the convolutional layers. These layers take in an image, maintaining the 2D structure, and convolve it with a number of kernels. Each kernel will produce a new image. Thankfully, PyTorch will do all the math and tracking for us. Just like we had `Linear` layers before, we can get a convolutional layer by adding a `Conv2D` to our model.\n",
    "\n",
    "As before, we'll use a `Sequential` model to keep track of how to build things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq = torch.nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a convolution layer, we need to provide it with three arguments: the number of channels it expects as input, the number of kernels to create, and the size of those kernels. We don't have to specify the size of the images, it'll figure that out. \n",
    "\n",
    "This first layer will be looking at our input images. They have three color channels, so we set `in_channels=3`. For our kernels, let's start with $16$ kernels of size $3$ x $3$. We specify the kernel size with `kernel_size=(3,3)`, and the number of kernels with `out_channels=16`. We'll also need padding to keep our image size. With a $3$ x $3$ kernel, we'll set padding to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3, 3), padding=1)\n",
    "model_seq.append(conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get a single batch of $32$ images, so we can see what the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(train_loader))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the batch is the shape we expect. It should be $32$ images, with $3$ color channels, of size $224$ x $224$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.10:** Get the shape of the batch and store it to `batch_shape`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shape = ...\n",
    "\n",
    "print(f\"Batch shape: {batch_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the model we have so far on the data. Since we only have the one convolutional layer, we'll see its output.\n",
    "\n",
    "This is a useful trick for making sure we're getting what we expect. What _do_ we expect here? We should still have $32$ images, but now they should have $16$ channels corresponding to the $16$ kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_step_out = model_seq(test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.11:** Get the shape of the output and store it to `first_step_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_step_shape = ...\n",
    "\n",
    "print(f\"Shape after first convolution layer: {first_step_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always with a network, we'll need an activation function. Here we'll use the ReLU like we did in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq.append(torch.nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "Be careful here. When we append to our Sequential model, we're modifying it. If we run the cell again, this will add a second ReLU layer. Not what we want! There are other ways to organize the model creation that don't have this problem. But the simplest thing would be to build your whole model in one cell. That way, when you re-run it, it starts from scratch.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our convolution layers transform images into more images. Ultimately, we're going to need to get down to just our $8$ output classes. But our convolution has _increased_ the number of pixels! Max pooling will let us shrink our image.\n",
    "\n",
    "In PyTorch, this is a `MaxPool2D` layer. The 2D is because we're leaving the channels alone, so it'll max pool on each of our $16$ channels separately. We'll need to say how big of a patch to reduce, called the kernel again. We'll set it to $2$ x $2$, a standard choice. We'll set our stride to $2$ as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool1 = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "model_seq.append(max_pool1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that this did what we wanted. It should have left us with $32$ images, with $16$ channels, but half as large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.12:** Run the current model on the `test_batch`, and save the output's shape to `max_pool_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool_out = ...\n",
    "max_pool_shape = ...\n",
    "\n",
    "print(f\"Shape after first max pool: {max_pool_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sequence of convolution, ReLU, max pool is very common in CNNs. Often networks will have several of these in a row. Let's add two more to ours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.13:** Add a convolution layer taking in our $16$ channels and outputting $32$ channels, with a $3$ x $3$ kernel and padding of $1$. Follow that with a ReLU, and a max pool of size $2$ x $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as always, let's verify the shape. What should we expect this time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.14:** Run the current model on the `test_batch`, and save the output's shape to `second_set_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_set_out = ...\n",
    "second_set_shape = ...\n",
    "\n",
    "print(f\"Shape after second max pool: {second_set_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If things went according to plan, you should now have $32$ channels and a $56$ x $56$ image.\n",
    "\n",
    "You can actually simplify these descriptions. First, you don't have to provide the argument names for most things, IF you provide them in the right order (`padding` is an exception). Second, for the kernels, if you say $2$ it knows you mean $2$ x $2$ (same for $3$, etc). Finally, for the max pool you can leave off the stride, it defaults to the size of the kernel. We can use this to make the description of our third layer set more compact. We'll use $64$ kernels this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3 = torch.nn.Conv2d(32, 64, 3, padding=1)\n",
    "max_pool3 = torch.nn.MaxPool2d(2)\n",
    "model_seq.append(conv3)\n",
    "model_seq.append(torch.nn.ReLU())\n",
    "model_seq.append(max_pool3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.15:** Run the current model on the `test_batch`. Save the output's shape to `third_set_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_set_out = ...\n",
    "third_set_shape = ...\n",
    "\n",
    "print(f\"Shape after third max pool: {third_set_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have a $32$ x $64$ x $28$ x $28$. We could keep adding more of these sets of layers, but this should be plenty. Now we need to move toward getting our final $8$ classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finishing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to need an output layer with just $8$ neurons. That's a flat output, without the 3D structure of our images. Conveniently, PyTorch provides a `Flatten` layer for flattening. Let's add that to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq.append(torch.nn.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What shape should we expect now? It should be our batch size of $32$ by however many pixels we had. This layer has just taken all our pixels and laid them out into a big vector. How many is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64 * 28 * 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, then we can use the model to make sure we've got it right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.16:** Run the current model on the `test_batch`. Save the output's shape to `flat_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_out = ...\n",
    "flat_shape = ...\n",
    "\n",
    "print(f\"Shape after flattening: {flat_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have a flat input, and can build a normal set of dense layers. You can think of the convolution/max pool layers as having done the image processing. Now we need to do the actual classification. It turns out that dense layers are good at that task.\n",
    "\n",
    "We could add a single layer and just go straight to our output $8$ classes. But we'll get better performance by adding a few dense layers, `Linear` in PyTorch's terminology, first. For these layers, we need to tell it the size of the input, and how many neurons we want in the layer. Since the input is our previous layer, we tell it that size. We'll add a layer of $500$ neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(in_features=50176, out_features=500)\n",
    "\n",
    "model_seq.append(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.17:** Add the ReLU activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we should be getting an output shape from the $500$ neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_out = model_seq(test_batch)\n",
    "linear_shape = linear_out.shape\n",
    "\n",
    "print(f\"Shape after linear layer: {linear_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could add more of these, but this network has already gotten rather large. Let's put in the final layer on. We'll need a dense layer with $8$ outputs. But this time we don't add the activation function. We need something different for the final layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.18:** Add the output dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = ...\n",
    "\n",
    "model_seq..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one final shape check. If everything has gone according to plan, we should be getting $8$ outputs for each of our input images, giving us a $32$ x $8$ tensor on our test batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq(test_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the model! We're finally done. Now we need to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, let's put all the model code in one place. This is how you'd do it in practice, to prevent errors.\n",
    "\n",
    "We have also added `Dropout` layers after the flattened and linear layers. This helps to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important! Don't change this\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "model = torch.nn.Sequential()\n",
    "\n",
    "conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3, 3), padding=1)\n",
    "max_pool1 = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "model.append(conv1)\n",
    "model.append(torch.nn.ReLU())\n",
    "model.append(max_pool1)\n",
    "\n",
    "conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "max_pool2 = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "model.append(conv2)\n",
    "model.append(torch.nn.ReLU())\n",
    "model.append(max_pool2)\n",
    "\n",
    "conv3 = torch.nn.Conv2d(32, 64, 3, padding=1)\n",
    "max_pool3 = torch.nn.MaxPool2d(2)\n",
    "model.append(conv3)\n",
    "model.append(torch.nn.ReLU())\n",
    "model.append(max_pool3)\n",
    "\n",
    "model.append(torch.nn.Flatten())\n",
    "model.append(torch.nn.Dropout())\n",
    "\n",
    "linear1 = torch.nn.Linear(in_features=50176, out_features=500)\n",
    "model.append(linear1)\n",
    "model.append(torch.nn.ReLU())\n",
    "model.append(torch.nn.Dropout())\n",
    "\n",
    "output_layer = torch.nn.Linear(500, 8)\n",
    "model.append(output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to make sure PyTorch has the model correct, let's look at the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = 224, 224\n",
    "summary(model, input_size=(batch_size, 3, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has over 25 million parameters. That's fewer than our previous model, but because of the more complicated architecture it'll take more time and resources to train.\n",
    "\n",
    "We can use the same training code we used last time. It can handle binary or multiclass classification. We made a separate file with this code in the previous notebook. Now we can reuse that code by importing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import predict, train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous notebook, we'll use the cross entropy as our loss function. This will take into account how confident the model is in its answer, as well as whether it was right or wrong. We will also print the accuracy as a human-readable measure.\n",
    "\n",
    "We'll need to set up our Cross Entropy loss, and an optimizer. We'll also make sure our model is on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And away we go! This is exactly the same code as last time, so it's called the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.19:** Train the model for $8$ epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\"> <strong>Regarding Model Training Times</strong>\n",
    "\n",
    "This task involves training a neural network for 8 epochs. As highlighted in the accompanying video, the training process is computationally intensive and can be very time-consuming. On most systems, each epoch may take between 10 and 15 minutes, meaning the entire training process could last well over one hour. In an online lab, this could result in timeouts or interruptions.\n",
    "To streamline your learning experience, where the video omits over an hour of training footage, we have provided a pre-trained model for your convenience. This model is an exact replica of the one you have been working on, trained for 8 epochs and carefully serialized using <code>torch.save()</code>.\n",
    "Upon completing the video for Task 1.4.19, you can proceed by loading the pre-trained model using the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pretrained Model:** Load the pre-trained model with the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"model/trained_model.pth\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "We've chosen to train for eight epochs. This was pretty arbitrary. We wanted to get a decent result without running for too long. If the validation loss is still decreasing, the model is still improving. In that case, we may benefit from training longer. As models get larger, they generally take longer to train. Thankfully, training more doesn't restart the process. We could just run the training function again and it'll pick up where it left off.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in the previous notebook, we can use our predict function to get the confidence values, and convert them to predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.20:** Calculate the predictions for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = ...\n",
    "predictions = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those and the correct answers, we can generate the confusion matrix. Let's pull the targets into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "\n",
    "for _, labels in tqdm(val_loader):\n",
    "    targets.extend(labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.21:** Make the same confusion matrix we made last time. You'll need to either move the `predictions` to `cpu` or convert them to a list. The labels will be our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ...\n",
    "\n",
    "disp = ...\n",
    "\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation=\"vertical\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're doing at least as well as our binary classification, but this time with many more animal choices. It seems some animals are a bit more difficult to distinguish from the `'blank'` images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission to competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our model to complete the competition. They want our model's confidences for each of the $8$ classes on a test set. We don't know the answer here (it wouldn't be much of a competition if we did!). We'll need to run each image through our model.\n",
    "\n",
    "They want the results saved as a csv file, with the columns labeled by the class, and the row index with a special ID. They've given us a file saying how these IDs map to image files. This will be easiest for us to save as a DataFrame.\n",
    "\n",
    "First, we need to read in the IDs and filenames. All of the test data is in the `data_multiclass/test` directory, and this ID mapping is in the `test_features.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.22:** Read in the ID mapping csv file with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "# Read the CSV file\n",
    "id_file_location = ...\n",
    "df_ids = ...\n",
    "\n",
    "df_ids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ID is really just the filename without the `.jpg` on the end, and all of the images are in the `test_features` subdirectory of the test directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our model on one of the images. We can get the file location from our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_location = df_ids.iloc[0, 1]\n",
    "test_image_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the location within our test folder. Let's open the image and look at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.23:** Use `PIL` to open this image and assign it to `test_image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = ...\n",
    "test_image = ...\n",
    "test_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is expecting images that have had our transformations applied. This is thankfully easy to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_trans = transform_norm(test_image)\n",
    "test_image_trans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there. Our model is actually expecting a batch of these. Since we only have one, we'll need to change the tensor to a $1$ x $3$ x $224$ x $224$. We could do this with `reshape`, but it's easier to use `unsqueeze`. `unsqueeze` is meant for this exact problem. It adds an extra dimension with one element. We just specify _which_ extra dimension we want. In our case, we want the first dimension (i.e. `0`) to be the extra one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unsqueeze = test_image_trans.unsqueeze(0)\n",
    "test_unsqueeze.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run our model on this. Since our model is on the GPU now, we'll need to move this image there too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_cuda = test_unsqueeze.to(device)\n",
    "test_out = model(test_image_cuda)\n",
    "test_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the model's predictions, but it's not in the form we expected. We want a set of numbers between $0$ and $1$, that sum to $1$. What we're seeing is the raw output of the last layer. To convert this to the confidences, we need to run it through a `SoftMax`. This is very much like the logistic or sigmoid you've seen before, except that it works with many inputs. The `dim=1` tells it each row is one prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_softmax = torch.nn.functional.softmax(test_out, dim=1)\n",
    "test_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our numbers, we need to convert to a DataFrame. The predictions are made in the same order as the classes in our dataset. We can't convert a tensor directly to a DataFrame, so we'll convert it to a list first. Then we can set the columns to our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_softmax.tolist())\n",
    "test_df.columns = dataset.classes\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also should set the index to the ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = df_ids.iloc[0, 0]\n",
    "test_df.index = [image_id]\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're ready to go. Let's put this into a function to make it easier. We'll also add a few things to make PyTorch run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "\n",
    "def file_to_confidence(file_path, image_id):\n",
    "    image = PIL.Image.open(file_path)\n",
    "    transformed = transform_norm(image)\n",
    "    unsqueezed = transformed.unsqueeze(0)\n",
    "    image_cuda = unsqueezed.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model_raw = model(image_cuda)\n",
    "        confidence = torch.nn.functional.softmax(model_raw, dim=1)\n",
    "\n",
    "    conf_df = pd.DataFrame(confidence.tolist())\n",
    "    conf_df.columns = dataset.classes\n",
    "    conf_df.index = [image_id]\n",
    "\n",
    "    return conf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the function works as expected. We should get the same result as before. Note that the function is expecting the full file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_confidence(test_image_path, image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this function on each file to get the predictions. We can loop over the ids and filenames and build up a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4.24:** Do it! You can use `df_ids.itertuples()` to get one row at a time, and `pd.concat` to assemble many DataFrames into one big one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the competition wants this as a csv. We can save a pandas DataFrame directly to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've put a lot of effort into this model, and it took a while to train. The training has determined what the best parameters (also called weights) for our network. The only information we need to reproduce it's the network architecture, and the values of those parameters. PyTorch lets us save all of this. Then we can just load the model in the future instead of having to retrain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model/deepnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good work! This was a long one, but we covered a lot of important ideas. We're now able to use PyTorch to classify any number of images, with any number of classes! Here are the key takeaways:\n",
    "\n",
    "- Some preprocessing of our images, or data in general, can help our models.\n",
    "- In multiclass classification, we get a confidence for each of our classes.\n",
    "- Neural networks can have different ways of connecting their neurons, called architectures\n",
    "- The Convolutional Neural Network architecture does well on images\n",
    "- In PyTorch, we can build one by choosing different layers\n",
    "- With a trained model, we can predict the class of an unlabeled image\n",
    "\n",
    "All together, this let us complete the competition we have been working on. We now have a model that gives us confidences (and thus predictions) on what animals appear in a camera trap image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "This file &#169; 2024 by [WorldQuant University](https://www.wqu.edu/) is licensed under [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06107b93e89a401f8f8120ca068ea212": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "065f68c72e4943fca033062c1e062b23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5c207d7f911b45b7bb971347e5d553b7",
       "style": "IPY_MODEL_eb8350c1132a4084a179d1488bd566b9",
       "value": "Scoring:  99%"
      }
     },
     "084db31d86734c2e95d9d2cf3f87a34f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a14d91cf5e9a4823a36ee5371f0cbc0e",
       "style": "IPY_MODEL_a3c79e2cda85472299431ad5d153e2ce",
       "value": " 103/104 [00:14&lt;00:00,  7.64it/s]"
      }
     },
     "0a4c5bf8b2ad4e7cac50e310c09880d3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0b12e7aef7ae42c98ff08eefe9352a1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "112cf40e0fb042b48f56a0af45b7786c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "115e815a75984f76baaeeb1a8428e0d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_112cf40e0fb042b48f56a0af45b7786c",
       "max": 104,
       "style": "IPY_MODEL_8aa9ede8a90d4d49b11bce095eb0ef9f",
       "value": 104
      }
     },
     "11c871f80bfb404b9b52d100dbb3fcdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "14e743fa7cee42b1b27203530c87bba5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "185b3dd4363c4ea39fe1bcb6734a649d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_533ec6aef6424e4dbb944d5bfe7e8fb5",
       "style": "IPY_MODEL_d9e1c19fae2142b79cdba4c280af083a",
       "value": "Training: 100%"
      }
     },
     "1aed6092f4434cd580b3b41833373ee9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "1e03e9f6ae604ff3a3a7b89421ca8d98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_537e8efe349545208fe4f94f624e5e82",
       "style": "IPY_MODEL_8f0f305348f344cd93f5d3f6c24c5557",
       "value": " 412/413 [01:19&lt;00:00,  5.27it/s]"
      }
     },
     "1f58037d141f4d1586b50bcd9840bf1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_affd7e69f75043049f5d0dce50fee0c2",
       "max": 413,
       "style": "IPY_MODEL_26348996a8c24e1aa6427a023eb605b0",
       "value": 413
      }
     },
     "20bd941aff024468aa1e6ca181ad7759": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_e6a49049fa9c44b1956053141fcc4fce",
        "IPY_MODEL_b9b94ea2d41b44529503083316e0bdcc",
        "IPY_MODEL_c4bc0748913345edbbbaf2d9111c659d"
       ],
       "layout": "IPY_MODEL_4bb675032ecf43deabfb01a4fdc0764e"
      }
     },
     "21cc54570619450ca70b0b8705dc138c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_31d574d48f5c437c9f7467a3f6fec4bd",
       "max": 413,
       "style": "IPY_MODEL_a89e5df309c24ff5a6faa4f7303760b9",
       "value": 413
      }
     },
     "230b5201fbd74bea91f5c87c2daf6320": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2476202d27ad470b9de776da6bfbd8d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0a4c5bf8b2ad4e7cac50e310c09880d3",
       "style": "IPY_MODEL_4ee225388a774ed0b47fbcf9f25cefdd",
       "value": " 412/413 [01:16&lt;00:00,  5.32it/s]"
      }
     },
     "26348996a8c24e1aa6427a023eb605b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2a3155e597cf44ec93c1ee8c3cec5f28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_86bc1b600d6147f9a53587e04a95f622",
       "max": 516,
       "style": "IPY_MODEL_55cf4e90d90546cd96ffa1f25d1e7c5d",
       "value": 516
      }
     },
     "2aa93b631b304f8fa56a880896b2de4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "2c9c80e5621540039fb4aff942748645": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "31d574d48f5c437c9f7467a3f6fec4bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "33d7cb8902474c46905b7d9e17aacc71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_7397d24c7c3e46ee9d15c186de56c9ab",
       "max": 104,
       "style": "IPY_MODEL_72a2ac5077fa4c0e8eb8eb3be0d5feef",
       "value": 104
      }
     },
     "36d8a95ba5844ced96abb3b748cfae8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "39502e67ae4747579004e97adcc61ef4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "39e5c72e2bad4445a2370fa4f6013f25": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3bac47cb167b4ac8a196fee583ed95cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_ac95793d70454f56add5149b3aa2b4c0",
       "max": 104,
       "style": "IPY_MODEL_14e743fa7cee42b1b27203530c87bba5",
       "value": 104
      }
     },
     "3d84c96ae9184543a62777c1e807f881": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8cd7d0d8e53f4053a89c594706b9a8a1",
       "style": "IPY_MODEL_5a2136354a7948e291fd71f5f0df162e",
       "value": "Training: 100%"
      }
     },
     "3f370b100fcb43d3a63af0592aa4427a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a2f4be128fe840998be404bd6f68f522",
       "style": "IPY_MODEL_36d8a95ba5844ced96abb3b748cfae8e",
       "value": " 103/104 [00:15&lt;00:00,  6.76it/s]"
      }
     },
     "415a3f92ae604143abbf061def857b20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "417dfa020fd0462b9bf61497be57c42c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "41a024477d52479ba5307c3a53536dcc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_7fe648f101104337850fd459bac4e253",
       "max": 413,
       "style": "IPY_MODEL_11c871f80bfb404b9b52d100dbb3fcdd",
       "value": 413
      }
     },
     "423265a12afd4e6aa3064d83fe6e0926": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "434cfbe771b3473cb80db9a34c2f4d71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "437ebef0435b409995af4d46107b8084": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f61dd81a84664b158ce2c884bb8b5b0b",
       "style": "IPY_MODEL_905d8dca7e1f4c0d910e5ec96393e0bc",
       "value": " 412/413 [01:16&lt;00:00,  5.67it/s]"
      }
     },
     "44ad406830564ce6aab3a6cea7d21043": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "466c4c14e98a4df183cdf073082a234d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_eb86e8d1f5684bc09ae3488b808ede13",
       "style": "IPY_MODEL_efc47a61b51f4a288f9e35154b2efe75",
       "value": " 412/413 [01:17&lt;00:00,  5.44it/s]"
      }
     },
     "4815d9e2ff004be09979070a7865f95c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "49040a79cd5b41dcab58477f91281855": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4bb675032ecf43deabfb01a4fdc0764e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4c34a18eb6164afaad161de0dc26f926": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4c995d7646e3463e8c5b5c4033a8d7bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4ee225388a774ed0b47fbcf9f25cefdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4effbc88f79641909f50398421d6be59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5075678f5a5246f6927f0ed24bf6d3fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "50c1f559637343b697a06597c37fe66e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "50cd4c42dc9747178459eee5b50e15bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_ba0c41b87e5a486eaba1de85166265f0",
       "max": 413,
       "style": "IPY_MODEL_4effbc88f79641909f50398421d6be59",
       "value": 413
      }
     },
     "51e8b0c4b0c9408d813e11f86a79fb75": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "524578720feb440ca70f189f433265f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "533ec6aef6424e4dbb944d5bfe7e8fb5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "537e8efe349545208fe4f94f624e5e82": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "541adfd5aeb544db9749a87238d3eca3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5447dea6ad384bc197a40d81fa76f1d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "54a99db4b6e14182a2d1f4da5fd06db8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "54e91a98cb894dc994de061732dfd120": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_423265a12afd4e6aa3064d83fe6e0926",
       "style": "IPY_MODEL_e2c77c78ddcf4cc2bf1e7172fc2af0f8",
       "value": " 412/413 [01:17&lt;00:00,  5.38it/s]"
      }
     },
     "55cf4e90d90546cd96ffa1f25d1e7c5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "58cccd6beb464d07b04fc1c10a44e2af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_39e5c72e2bad4445a2370fa4f6013f25",
       "style": "IPY_MODEL_d2525ab5821b4813bdb98d1c1d6fc04c",
       "value": "Training: 100%"
      }
     },
     "58f3f13b13534ce6bc7475234725b7bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "5905a77b2f464bb98922e0abb3d9ca40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5a2136354a7948e291fd71f5f0df162e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5b53a240a7944512a428dbb11cd76094": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5c207d7f911b45b7bb971347e5d553b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5db491adba494dc8bd5f96c46d801675": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5eb35b21c5d44bf098786406f29d2065": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5f27bf66746347c1b98354ecc919f1e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_98d75ae0d2e743ac86dda19a0946320d",
       "style": "IPY_MODEL_f7ed47dc6ef14721a19dc2b23f65c18f",
       "value": " 412/413 [01:18&lt;00:00,  5.14it/s]"
      }
     },
     "6524bdd1796d42809adba71a0ff65238": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ee0c276c77724b5aaca1146be42b14d5",
       "style": "IPY_MODEL_7427fa9f6d25427581cbd7e39e262301",
       "value": "Computing mean and std: 100%"
      }
     },
     "6761984fd3ee436a8da75f3d78264ac6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "67cd776c1b864558942faf9b5f6c28f7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "67da9b7e9d274512914050bbd83b43d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_7d2fdd6ab46b47428418de02569320a6",
       "max": 104,
       "style": "IPY_MODEL_6c64ccf503304a8d9d825877593c2009",
       "value": 104
      }
     },
     "682511b66d664df2a9a13a118624fcfc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_39502e67ae4747579004e97adcc61ef4",
       "style": "IPY_MODEL_b5a19e646c5f42c691d4792e4acf0cb3",
       "value": " 103/104 [00:15&lt;00:00,  7.04it/s]"
      }
     },
     "683e400bb1e1449d9695c44230e0efef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6867bd4d83544277bef8661f5a42cd47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f6239f86576e49d8b6b9c262b2569020",
       "style": "IPY_MODEL_ed06f8379fe648d3bf51400f1db11aad",
       "value": "Training: 100%"
      }
     },
     "6b6d4c97e92b4c4b8da3eed8d683eb86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_d9546966aa49471db7d847e5e4d93b54",
       "max": 413,
       "style": "IPY_MODEL_ed19d0bcf23745b6a0f16dd1b3027753",
       "value": 413
      }
     },
     "6c64ccf503304a8d9d825877593c2009": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6c75c6babd3e4ba194fed8a52af8b80e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6ef2aa5a51254529a2b076b5cabd465d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "72468c0e3e2f41dc937f3dce5f3eed43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a5ea12c0f4904726943670f620dbbaa5",
       "style": "IPY_MODEL_79f1e8726eda4e458f9d17bba585a917",
       "value": "Training: 100%"
      }
     },
     "72a2ac5077fa4c0e8eb8eb3be0d5feef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7397d24c7c3e46ee9d15c186de56c9ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "73cd638f9b2d4e9d99249a983c4a9baa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7427fa9f6d25427581cbd7e39e262301": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "76e838026ace40c2a5dbf40dd31e9c8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d8965ad21d8a457faad993766212c764",
       "style": "IPY_MODEL_ca4fa0655d0f4bf1b54765d04d3c1594",
       "value": "Scoring:  99%"
      }
     },
     "771515fef8ce41508c816d80a4328d80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "79451f586c9441c5a353c5ade48f2051": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_541adfd5aeb544db9749a87238d3eca3",
       "style": "IPY_MODEL_db151227850f48bd9b163705114faeee",
       "value": "Training: 100%"
      }
     },
     "79f1e8726eda4e458f9d17bba585a917": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7a4aeedf486d41549b171a1a4b0a30b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_73cd638f9b2d4e9d99249a983c4a9baa",
       "style": "IPY_MODEL_2c9c80e5621540039fb4aff942748645",
       "value": "Scoring:  99%"
      }
     },
     "7d2fdd6ab46b47428418de02569320a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7d506d1c1a9d49d4ad8924d1d939f437": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7eb11bc4438f411ebc404ea479e7b297": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "7fe648f101104337850fd459bac4e253": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "83484c1cccab41f2bbd91c457689879f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e42c10e3583647b382135e61502995ea",
       "style": "IPY_MODEL_adfef37e0c4f4bf09117c8dd8ea96bea",
       "value": "Predicting:  99%"
      }
     },
     "85778bb6e6b14f409539e827565790c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_44ad406830564ce6aab3a6cea7d21043",
       "max": 413,
       "style": "IPY_MODEL_5075678f5a5246f6927f0ed24bf6d3fd",
       "value": 413
      }
     },
     "86bc1b600d6147f9a53587e04a95f622": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "87330f77f6e74c58b98d4c01b75d1c2f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "874c37db69754e099268e31bccda3c99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b7adc1a08f304cf7b44aeb07a93a207f",
       "style": "IPY_MODEL_c86aa84a1087476fbcc38c62a3c96ae2",
       "value": "Scoring:  99%"
      }
     },
     "87e441951ae04cb9ab8f2f285a816b59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ccb7496046184cea86672d2617b4c5d0",
       "style": "IPY_MODEL_9ac143c68cc1464da2f42b314f02c372",
       "value": "Scoring:  99%"
      }
     },
     "89a16d29beb64606a05e62b24bef3e07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_230b5201fbd74bea91f5c87c2daf6320",
       "style": "IPY_MODEL_bf1a6a82b3994bc0a1df98adf3e40edf",
       "value": "Scoring:  99%"
      }
     },
     "8aa9ede8a90d4d49b11bce095eb0ef9f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8cd7d0d8e53f4053a89c594706b9a8a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8d65d58357aa434bb81a0f0281b9de40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8f0f305348f344cd93f5d3f6c24c5557": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "905d8dca7e1f4c0d910e5ec96393e0bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "91c66721b1d34379851b5d87e8aec9b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_de1a64b996ea4849a277a8ec9158a501",
       "max": 104,
       "style": "IPY_MODEL_f85f5d6c50984ae69127794adf1c212f",
       "value": 104
      }
     },
     "963d28f5ef404bf085a42fd697a787dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "98d75ae0d2e743ac86dda19a0946320d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "99107f3e572e4400a8f0f4bfd3dd54b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_5b53a240a7944512a428dbb11cd76094",
       "max": 104,
       "style": "IPY_MODEL_4c995d7646e3463e8c5b5c4033a8d7bb",
       "value": 104
      }
     },
     "9aa8d5ff63db4203a370c947ff5c64d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9ac143c68cc1464da2f42b314f02c372": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9e1a333d48c447ab96ccfa3d153a9608": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_7d506d1c1a9d49d4ad8924d1d939f437",
       "max": 104,
       "style": "IPY_MODEL_54a99db4b6e14182a2d1f4da5fd06db8",
       "value": 104
      }
     },
     "9fc30a85273445f1b5fc523ff43f9ca0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b5aa96d46cd246aa864682a0cbc78180",
       "style": "IPY_MODEL_5db491adba494dc8bd5f96c46d801675",
       "value": "Training: 100%"
      }
     },
     "a14d91cf5e9a4823a36ee5371f0cbc0e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a2ef17468e764da0ad9ef2015102d650": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "a2f4be128fe840998be404bd6f68f522": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a3c79e2cda85472299431ad5d153e2ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a5891695e40148839dfd0d50c8de21bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a5ea12c0f4904726943670f620dbbaa5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a89e5df309c24ff5a6faa4f7303760b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a8d1cdbd81b5430dabf158086aee8638": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "aafb160a54a043298a1f571329b0ff22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ac95793d70454f56add5149b3aa2b4c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ad1d59fa35f94f1d95fbfb73de579251": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c91b8f4b0994458c97558ee67b86fceb",
       "style": "IPY_MODEL_d08c78a49c4747369a8c167b5c5138c9",
       "value": "Scoring:  99%"
      }
     },
     "ad5a5740ff3e4166994cef3cec78cb0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5905a77b2f464bb98922e0abb3d9ca40",
       "style": "IPY_MODEL_c0f39c294b2544b5852db1b73c5d9a29",
       "value": "Training: 100%"
      }
     },
     "adfef37e0c4f4bf09117c8dd8ea96bea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "affd7e69f75043049f5d0dce50fee0c2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b07c88e4a2d6456baeb7603549be693f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "b424c669ddb947a08970bf53eaf66d3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f25649293d1043c5a23dae884c0d3f5a",
       "style": "IPY_MODEL_b8851931fdd740beb70082543e08114e",
       "value": " 103/104 [00:15&lt;00:00,  7.12it/s]"
      }
     },
     "b5a19e646c5f42c691d4792e4acf0cb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b5aa96d46cd246aa864682a0cbc78180": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b5aea266115a41b5b27d7675a99bca5a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b6cb5ea6b60049d394f03a08b9456505": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b742184fafbd4790a8582419622abc09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_8d65d58357aa434bb81a0f0281b9de40",
       "max": 104,
       "style": "IPY_MODEL_a5891695e40148839dfd0d50c8de21bc",
       "value": 104
      }
     },
     "b7adc1a08f304cf7b44aeb07a93a207f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b8851931fdd740beb70082543e08114e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b9b94ea2d41b44529503083316e0bdcc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_bc4d132279914322b4666f2ee3187d8a",
       "max": 104,
       "style": "IPY_MODEL_6c75c6babd3e4ba194fed8a52af8b80e",
       "value": 104
      }
     },
     "ba0c41b87e5a486eaba1de85166265f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bc4d132279914322b4666f2ee3187d8a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bf1a6a82b3994bc0a1df98adf3e40edf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bfc7d54d2313465cb79ce8ad11fcc18f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c041c3f65fa048b48acca783263185f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "c0f39c294b2544b5852db1b73c5d9a29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c215105dfdc64f3bb4ce10d34aaef415": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b6cb5ea6b60049d394f03a08b9456505",
       "style": "IPY_MODEL_ef5b31ef86a642f2b4fb4e41e9c799c1",
       "value": " 103/104 [00:15&lt;00:00,  7.10it/s]"
      }
     },
     "c4bc0748913345edbbbaf2d9111c659d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9aa8d5ff63db4203a370c947ff5c64d2",
       "style": "IPY_MODEL_e063ee2fd9494611b2605965a59db70e",
       "value": " 104/104 [00:13&lt;00:00,  8.13it/s]"
      }
     },
     "c86aa84a1087476fbcc38c62a3c96ae2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c91b8f4b0994458c97558ee67b86fceb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ca4fa0655d0f4bf1b54765d04d3c1594": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ccb7496046184cea86672d2617b4c5d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ccccf2630c0e4e1a90f1fe9f73de6fde": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4c34a18eb6164afaad161de0dc26f926",
       "style": "IPY_MODEL_4815d9e2ff004be09979070a7865f95c",
       "value": " 103/104 [00:15&lt;00:00,  6.93it/s]"
      }
     },
     "d08c78a49c4747369a8c167b5c5138c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d0b4207411ee400aad9011a5a52197b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d1b8982d6e7c41f98dc829a46baf9e31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e1273af9bc484226be38419afb153028",
       "style": "IPY_MODEL_d32c36e3a8f4432c858b2166bc73a48d",
       "value": " 103/104 [00:15&lt;00:00,  6.83it/s]"
      }
     },
     "d2525ab5821b4813bdb98d1c1d6fc04c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d2aa2bed195d4306806e147b841d8fd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_87330f77f6e74c58b98d4c01b75d1c2f",
       "max": 413,
       "style": "IPY_MODEL_e0ef60e859854e5e80f0cb64d4eda895",
       "value": 413
      }
     },
     "d32c36e3a8f4432c858b2166bc73a48d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d58bddb4007245e992bc56428ef4f0e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_51e8b0c4b0c9408d813e11f86a79fb75",
       "max": 413,
       "style": "IPY_MODEL_fdbff2778d9d4a038f9402473c8299cc",
       "value": 413
      }
     },
     "d7df2ee444e74bd08f40e9e0bdd22e97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_683e400bb1e1449d9695c44230e0efef",
       "style": "IPY_MODEL_b5aea266115a41b5b27d7675a99bca5a",
       "value": " 515/516 [01:06&lt;00:00,  7.35it/s]"
      }
     },
     "d8965ad21d8a457faad993766212c764": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d8b977163b7940a7b026fc600c039e40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "d9546966aa49471db7d847e5e4d93b54": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d9e1c19fae2142b79cdba4c280af083a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "db151227850f48bd9b163705114faeee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "de1a64b996ea4849a277a8ec9158a501": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dea0e455a8814010af858bf34dbcc672": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e063ee2fd9494611b2605965a59db70e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e0ef60e859854e5e80f0cb64d4eda895": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e11c7a66d3f44430b01484e26bf89b5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "e1273af9bc484226be38419afb153028": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e2c77c78ddcf4cc2bf1e7172fc2af0f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e3c3b462abc744b5807affc8b4ac61b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_67cd776c1b864558942faf9b5f6c28f7",
       "style": "IPY_MODEL_aafb160a54a043298a1f571329b0ff22",
       "value": "Scoring:  99%"
      }
     },
     "e42c10e3583647b382135e61502995ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6a49049fa9c44b1956053141fcc4fce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_bfc7d54d2313465cb79ce8ad11fcc18f",
       "style": "IPY_MODEL_0b12e7aef7ae42c98ff08eefe9352a1c",
       "value": "100%"
      }
     },
     "e6d7c4995d9d4547badd475b1d63e1de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_417dfa020fd0462b9bf61497be57c42c",
       "style": "IPY_MODEL_5447dea6ad384bc197a40d81fa76f1d2",
       "value": " 103/104 [00:15&lt;00:00,  7.14it/s]"
      }
     },
     "e79f1e178caf4896a236b7b7785e2baa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_50c1f559637343b697a06597c37fe66e",
       "style": "IPY_MODEL_49040a79cd5b41dcab58477f91281855",
       "value": " 103/104 [00:15&lt;00:00,  7.04it/s]"
      }
     },
     "e92afc2594df48c9b033606ed25a77f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "eb8350c1132a4084a179d1488bd566b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eb86e8d1f5684bc09ae3488b808ede13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ec9b9e8c270d452db7fc1af8ef218b51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6761984fd3ee436a8da75f3d78264ac6",
       "style": "IPY_MODEL_771515fef8ce41508c816d80a4328d80",
       "value": " 412/413 [01:16&lt;00:00,  5.24it/s]"
      }
     },
     "ed06f8379fe648d3bf51400f1db11aad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ed19d0bcf23745b6a0f16dd1b3027753": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ee0c276c77724b5aaca1146be42b14d5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "eef10f9fa8eb47528cb9c217855dd401": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "ef5b31ef86a642f2b4fb4e41e9c799c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "efc47a61b51f4a288f9e35154b2efe75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f180cacc8e6c4410b3d2b8ebb4b55504": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_5eb35b21c5d44bf098786406f29d2065",
       "max": 104,
       "style": "IPY_MODEL_f377ce5b2e9f4e2aa3e43ac883768747",
       "value": 104
      }
     },
     "f25649293d1043c5a23dae884c0d3f5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f377ce5b2e9f4e2aa3e43ac883768747": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f61dd81a84664b158ce2c884bb8b5b0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f6239f86576e49d8b6b9c262b2569020": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f7ed47dc6ef14721a19dc2b23f65c18f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f85f5d6c50984ae69127794adf1c212f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "fbbf1096b14f43f09c6ac73d6f4fe050": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_dea0e455a8814010af858bf34dbcc672",
       "style": "IPY_MODEL_d0b4207411ee400aad9011a5a52197b6",
       "value": " 412/413 [01:18&lt;00:00,  5.37it/s]"
      }
     },
     "fdbff2778d9d4a038f9402473c8299cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
